{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6821552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-09 17:38:05--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
      "dl.fbaipublicfiles.com (dl.fbaipublicfiles.com) をDNSに問いあわせています... 3.173.197.128, 3.173.197.49, 3.173.197.101, ...\n",
      "dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.173.197.128|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 2564550879 (2.4G) [binary/octet-stream]\n",
      "`sam_vit_h_4b8939.pth' に保存中\n",
      "\n",
      "sam_vit_h_4b8939.pt 100%[===================>]   2.39G  9.79MB/s 時間 4m 20s     \n",
      "\n",
      "2025-08-09 17:42:25 (9.40 MB/s) - `sam_vit_h_4b8939.pth' へ保存完了 [2564550879/2564550879]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e162322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ MPS detected: Patching apply_coords to return float32\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Table--154-_jpg.rf.67264c7a7b156dd01c44e5e35de6cbe9_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Table--154-_jpg.rf.67264c7a7b156dd01c44e5e35de6cbe9_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Sofa--348-_jpg.rf.74a1bda29972fc468b85d6c9eab48bbd_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Sofa--348-_jpg.rf.74a1bda29972fc468b85d6c9eab48bbd_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Table--149-_jpg.rf.b4c286bc8334135bbf9221d83ee9ed96_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Table--149-_jpg.rf.b4c286bc8334135bbf9221d83ee9ed96_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--5-_jpg.rf.68493a0c2f18c18e94687da77df7adeb_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--5-_jpg.rf.68493a0c2f18c18e94687da77df7adeb_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Sofa--350-_jpg.rf.afca5f82670c13bd4afbda3cbc386b2a_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Sofa--350-_jpg.rf.afca5f82670c13bd4afbda3cbc386b2a_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--3-_jpg.rf.dbf2a07925fc82e1cbebe13695f6a7da_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--3-_jpg.rf.dbf2a07925fc82e1cbebe13695f6a7da_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--2-_jpg.rf.53703002ffac527a3c2d19b775620bd2_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--2-_jpg.rf.53703002ffac527a3c2d19b775620bd2_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--6-_jpg.rf.5dc1508126a371ece989fbd1a753cfad_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--6-_jpg.rf.5dc1508126a371ece989fbd1a753cfad_sam_cropped.png\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--4-_jpg.rf.8654bfe9517687699936094bc8329910_sam_cropped.png\n",
      "SAM segmentation completed!\n",
      "✅ Saved: /Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip/Chair--4-_jpg.rf.8654bfe9517687699936094bc8329910_sam_cropped.png\n",
      "SAM segmentation completed!\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from types import MethodType  # 追加\n",
    "import clip  # 追加\n",
    "import PIL.Image  # 追加\n",
    "\n",
    "# input/output directories\n",
    "input_dir = Path(\"/Users/Kota/blended/Team3AmazonProject/data/temp/original\")\n",
    "output_dir = Path(\"/Users/Kota/blended/Team3AmazonProject/data/temp/cropped_sam_with_clip\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# SAM model\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# For CLIP\n",
    "clip_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=clip_device)\n",
    "clip_labels = [\"sofa\", \"chair\", \"table\"]\n",
    "\n",
    "# For MPS compatibility\n",
    "if device == \"mps\":\n",
    "    print(\"⚠️ MPS detected: Patching apply_coords to return float32\")\n",
    "    orig_apply_coords = mask_generator.predictor.transform.apply_coords\n",
    "\n",
    "    def apply_coords_float32(self, coords, size):\n",
    "        out = orig_apply_coords(coords, size)\n",
    "        # numpy配列のままfloat32へ\n",
    "        return out.astype(np.float32)\n",
    "\n",
    "    mask_generator.predictor.transform.apply_coords = MethodType(\n",
    "        apply_coords_float32, mask_generator.predictor.transform\n",
    "    )\n",
    "\n",
    "# 画像拡張子\n",
    "image_exts = {\".jpg\", \".png\"}\n",
    "\n",
    "for img_path in input_dir.glob(\"*\"):\n",
    "    if img_path.suffix.lower() not in image_exts:\n",
    "        continue\n",
    "\n",
    "    original = cv2.imread(str(img_path))\n",
    "    if original is None:\n",
    "        print(f\"⚠️ Failed to read: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    # CLIPでsofa/chair/table判定\n",
    "    pil_img = PIL.Image.fromarray(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "    image_input = clip_preprocess(pil_img).unsqueeze(0).to(clip_device)\n",
    "    text_inputs = clip.tokenize(clip_labels).to(clip_device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_inputs)\n",
    "        logits_per_image, _ = clip_model(image_input, text_inputs)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n",
    "    # いずれかのラベルが0.5以上ならマスク処理\n",
    "    if probs.max() < 0.5:\n",
    "        print(f\"⏩ Skip (not sofa/chair/table): {img_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # SAMはRGBのuint8でOK。float32化は不要・逆効果\n",
    "    image_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # マスク生成\n",
    "    masks = mask_generator.generate(image_rgb)\n",
    "    if not masks:\n",
    "        print(f\"❌ No mask detected: {img_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # 一番大きいマスクを採用\n",
    "    largest_mask = max(masks, key=lambda x: x[\"area\"])\n",
    "    mask = largest_mask[\"segmentation\"].astype(np.uint8)\n",
    "\n",
    "    # マスク適用（黒背景）\n",
    "    mask_3c = np.stack([mask] * 3, axis=-1)\n",
    "    masked_img = original * mask_3c\n",
    "\n",
    "    save_path = output_dir / f\"{img_path.stem}_sam_cropped.png\"\n",
    "    cv2.imwrite(str(save_path), masked_img)\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "\n",
    "print(\"SAM segmentation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team3amazonproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
